---
title: "Text mining HTM"
output: html_document
date: "2025-05-28"
---

## 1. INTRODUCTORY STEPS

We first setup the necessary packages for our analysis.
```{r setup}
# Set the CRAN mirror:
local({r <- getOption("repos")
r["CRAN"] <- "https://cran.rstudio.com/"
options(repos = r)})

# Install the packages used in this tutorial:
packages <- c("cld2", "quanteda", "seededlda", "sentimentr")

for (i in packages) {
    if(!require(i, character.only = TRUE)) {
        install.packages(i, dependencies = TRUE)
    }
}
```

Then, we can load and read the two data sets, the first regarding the BIC Cristal and the second regarding the ReMarkable 2 pen.
```{r  loading data}
bic_cristal_reviews <- read.csv("bic_cristal_reddit.csv")
remarkable2_pen_reviews <- read.csv("remarkable2_pen_reddit.csv")
```


## 2. DATA EXPLORATION AND PREPARATION

At this point, we can start to explore, clean and prepare our two data sets, lets start by exploring their structure. First for the BIC Cristal data set.
```{r explore structure of bic cristal}
str(bic_cristal_reviews)
```
Then, for the ReMarkable 2 pen.
```{r explore structure of remarkable 2 pen}
str(remarkable2_pen_reviews)
```

Now, we need to check if the 'review_id' variable can be used as a UID for the different reviews in the data sets.
```{r UID verification}
length(unique(bic_cristal_reviews$review_id))
length(unique(remarkable2_pen_reviews$review_id))
```

As we can observe, the length of the vector containing unique values of the 'review_id' variable doesn't match the number of observations in the data sets, this means that a quite big number of different reviews has the same 'review_id' value, thus, we need to create a different variable that can be used as a UID. To do so, since each row contains a unique document, in the following chunk we will use the 'rownames' function. Then, to ensure consistency in the data sets, we convert the UID to a character.
```{r UID creation}
bic_cristal_reviews$UID <- row.names(bic_cristal_reviews)
remarkable2_pen_reviews$UID <- row.names(remarkable2_pen_reviews)

# Convert UID to a character varable
bic_cristal_reviews$UID <- as.character(bic_cristal_reviews$UID)
remarkable2_pen_reviews$UID <- as.character(remarkable2_pen_reviews$UID)
```

Now, since we observed that the variable 'review_id' is not useful to identify the reviews, we can remove it from both data sets. Then, we move the 'UID' column as the first one.
```{r 'review_id' removal}
# Remove 'review_id' column from both datasets
bic_cristal_reviews$review_id <- NULL
remarkable2_pen_reviews$review_id <- NULL

# Reorder columns to move 'UID' to the first position
bic_cristal_reviews <- bic_cristal_reviews[, c("UID", setdiff(names(bic_cristal_reviews), "UID"))]
remarkable2_pen_reviews <- remarkable2_pen_reviews[, c("UID", setdiff(names(remarkable2_pen_reviews), "UID"))]

# Confirm removal and column order
names(bic_cristal_reviews)
names(remarkable2_pen_reviews)
```

At this point, we remove observations for which the 'review' variable contains a missing value (if there's any).
```{r}
bic_cristal_reviews <- bic_cristal_reviews[ !is.na(bic_cristal_reviews$review), ]
remarkable2_pen_reviews <- remarkable2_pen_reviews[ !is.na(remarkable2_pen_reviews$review), ]
```

Moreover, since for sentiment analysis the tools are language specific, we verify that all reviews are in English, and, if they are not, we remove the ones written in other languages
```{r assign language}
# For the BIC Cristal
bic_cristal_reviews$language <- cld2::detect_language(bic_cristal_reviews$review)
head(cbind(table(bic_cristal_reviews$language)), 20)

# For the ReMarkable 2 pen
remarkable2_pen_reviews$language <- cld2::detect_language(remarkable2_pen_reviews$review)
head(cbind(table(remarkable2_pen_reviews$language)), 20)
```

As we can see, all reviews seem to be written in English, but for the BIC data set, it looks like 6 out of the 185 reviews cannot be classified, probably because they contain elements that disturb the functioning of the 'cld2' function (e.g. numeric values, symbols,..), furthermore, false negatives might exist.

Now, we remove (if there's any) the characters that are not in the standard ASCII range. This cleans the texts of the reviews and makes them more suitable to be elaborated by a computer. We start by marking those reviews that contain non-ASCII characters, then, if needed, we find-and-replace those characters with ones that are in the ASCII range.
```{r non_ascii}
# BIC Cristal
bic_cristal_reviews$non_ascii <- grepl("[^\x01-\x7F]", bic_cristal_reviews$review) # Mark reviews of interest  
bic_cristal_reviews[ which(bic_cristal_reviews$non_ascii %in% TRUE)[30], ]$review

# ReMarkable 2 pen
remarkable2_pen_reviews$non_ascii <- grepl("[^\x01-\x7F]", remarkable2_pen_reviews$review) # Mark reviews of interest  
remarkable2_pen_reviews[ which(remarkable2_pen_reviews$non_ascii %in% TRUE)[30], ]$review
```

As we can see, there's no reviews that contain non-ASCII characters.

## 3. MODEL PREPARATION

The first step is creating corpus for both data set, BIC Cristal and ReMarkable 2 pen. A corpus is a collection of texts that we can analyze. In this case, we will create a corpus for each data set using the 'quanteda' package. We will use the 'UID' variable as the document identifier and the 'review' variable as the text field.
```{r creation of corpus}
#BIC Cristal
corpusbic <- quanteda::corpus(bic_cristal_reviews, 
                           docid_field = "UID",
                           text_field = "review")

summary(corpusbic, 10)

#Remarkable 2 Pen
corpusRM <- quanteda::corpus(remarkable2_pen_reviews, 
                           docid_field = "UID",
                           text_field = "review")

summary(corpusRM, 10)
```
To check our corpus, we can randomly draw a review from each corpus and print it.
```{r corpus random draw}
as.character(corpusbic[ sample(length(corpusbic), 1) ])
as.character(corpusRM[ sample(length(corpusRM), 1) ])
```
Now, we can proceed to the sentiment analysis. We will use the 'sentimentr' package to calculate the sentiment scores for each review in both corpora. The sentiment scores will be stored in a new variable called 'ave_sentiment' in the document variables of each corpus. Furthermore, a subjectivity score is introduced (ranging from [0,1]) to the determine how subjective is the review, with a value close to zero indicating high objectivity and a value close to one indicating high subjectivity.
```{r sentriment analysis}
library(sentimentr)

# Enhanced sentiment analysis
analyze_sentiment <- function(corpus) {
    sentences <- get_sentences(as.character(corpus))
    sentiment <- sentiment_by(sentences)
    
    # Robust subjectivity calculation with diagnostics
    subjectivity <- lapply(sentences, function(x) {
        tryCatch({
            sentiment_detail <- sentimentr::sentiment(get_sentences(x))
            
            # Handle empty results
            if (nrow(sentiment_detail) == 0) return(0)
            
            # Calculate true subjectivity
            abs_weight_sum <- sum(abs(sentiment_detail$sentiment), na.rm = TRUE)
            word_count <- sum(sentiment_detail$word_count, na.rm = TRUE)
            
            # Prevent division by zero
            if (word_count == 0) return(0)
            
            subjectivity_score <- abs_weight_sum / word_count
            
            # Apply boost for opinion indicators
            opinion_indicators <- c("think", "believe", "feel", "opinion", "seem", "appear")
            if (any(opinion_indicators %in% unlist(get_sentences(x)))) {
                subjectivity_score <- min(1, subjectivity_score * 1.5)
            }
            
            return(subjectivity_score)
        }, error = function(e) {
            message("Error processing: ", x)
            0  # Return 0 on error
        })
    })
    
    subjectivity <- unlist(subjectivity)
    
    # Apply non-linear scaling to enhance differentiation
    scaled_subjectivity <- subjectivity^0.5  # Square root scaling
    
    return(list(sentiment = sentiment, subjectivity = scaled_subjectivity))
}

# BIC Cristal Sentiment + Subjectivity
bic_analysis <- analyze_sentiment(corpusbic)
docvars(corpusbic, field = "ave_sentiment") <- bic_analysis$sentiment$ave_sentiment
docvars(corpusbic, field = "subjectivity") <- bic_analysis$subjectivity

# Remarkable 2 Pen Sentiment + Subjectivity
rm_analysis <- analyze_sentiment(corpusRM)
docvars(corpusRM, field = "ave_sentiment") <- rm_analysis$sentiment$ave_sentiment
docvars(corpusRM, field = "subjectivity") <- rm_analysis$subjectivity

# Add to original data frames
bic_cristal_reviews$polarity <- bic_analysis$sentiment$ave_sentiment
bic_cristal_reviews$subjectivity <- bic_analysis$subjectivity
remarkable2_pen_reviews$polarity <- rm_analysis$sentiment$ave_sentiment
remarkable2_pen_reviews$subjectivity <- rm_analysis$subjectivity

# Handle any remaining NAs (shouldn't occur, but safeguard)
bic_cristal_reviews$subjectivity[is.na(bic_cristal_reviews$subjectivity)] <- 0
remarkable2_pen_reviews$subjectivity[is.na(remarkable2_pen_reviews$subjectivity)] <- 0
```

Now we can verify this sentiment analysis, this is achieved in the following chunk.
```{r}
# Verify new columns
cat("BIC Cristal columns:", names(bic_cristal_reviews), "\n")
cat("ReMarkable 2 Pen columns:", names(remarkable2_pen_reviews), "\n")

# Summary of new metrics
cat("\nBIC Cristal Sentiment & Subjectivity:\n")
print(summary(bic_cristal_reviews[, c("polarity", "subjectivity")]))

cat("\nReMarkable 2 Pen Sentiment & Subjectivity:\n")
print(summary(remarkable2_pen_reviews[, c("polarity", "subjectivity")]))

# Plot distributions
par(mfrow = c(2, 2))
hist(bic_cristal_reviews$polarity, main = "BIC Polarity", xlab = "Sentiment", col = "lightblue")
hist(bic_cristal_reviews$subjectivity, main = "BIC Subjectivity", xlab = "Subjectivity", col = "lightgreen")
hist(remarkable2_pen_reviews$polarity, main = "ReMarkable Polarity", xlab = "Sentiment", col = "lightblue")
hist(remarkable2_pen_reviews$subjectivity, main = "ReMarkable Subjectivity", xlab = "Subjectivity", col = "lightgreen")
par(mfrow = c(1, 1))
```

As we can see, BIC Cristal reviews are more objective, on average, compared to ReMarkable pen reviews.
Then, we can also check if the length of the sentiment scores (polarity and subjectivity) matches the number of documents in each corpus. 
```{r length check}
# Verify sentiment vector lengths
cat("BIC Cristal Sentiment Verification:\n")
cat(" - Corpus documents:", ndoc(corpusbic), "\n")
cat(" - Sentiment vector in corpus:", length(docvars(corpusbic, "ave_sentiment")), "\n")
cat(" - Polarity in data frame:", length(bic_cristal_reviews$polarity), "\n")

cat("\nReMarkable 2 Pen Sentiment Verification:\n")
cat(" - Corpus documents:", ndoc(corpusRM), "\n")
cat(" - Sentiment vector in corpus:", length(docvars(corpusRM, "ave_sentiment")), "\n")
cat(" - Polarity in data frame:", length(remarkable2_pen_reviews$polarity), "\n")

# Proper equality checks
bic_sent_ok <- length(docvars(corpusbic, "ave_sentiment")) == ndoc(corpusbic)
rm_sent_ok <- length(docvars(corpusRM, "ave_sentiment")) == ndoc(corpusRM)

cat("\nResults:\n")
cat("BIC sentiment length matches documents:", bic_sent_ok, "\n")
cat("ReMarkable sentiment length matches documents:", rm_sent_ok, "\n")
```
The results are TRUE, meaning that the sentiment scores have been successfully calculated and stored in the document variables of each corpus.

Now, we can explore the sentiment scores of both corpora. We can plot the distribution of the sentiment scores using histograms.
```{r plot sentiment scores}
#BIC Cristal
hist(quanteda::docvars(corpusbic)$ave_sentiment)
#Remarkable 2 Pen
hist(quanteda::docvars(corpusRM)$ave_sentiment)
```
As we can see, BIC Cristal has a skewed right histogram, indicating tendency for positive sentiment. While Remarkable 2 Pen has a more balanced distribution, with a slight tendency for positive sentiment as well.

Let's explore the sentiment scores further by checking the minimum and maximum values, as well as the mean and median.
```{r}
#BIC Cristal
summary(quanteda::docvars(corpusbic)$ave_sentiment)
#Remarkable 2 Pen
summary(quanteda::docvars(corpusRM)$ave_sentiment)
```
The value indicates that sentiment for both are positive, with BIC Cristal having a mean of 0.25 and a median of 0.28, while Remarkable 2 Pen has a mean of 0.16 and a median of 0.18. Let's explore sentiments with other method.
```{r explore sentiments}
#BIC Cristal
as.character(corpusbic[ which(quanteda::docvars(corpusbic)$ave_sentiment == summary(quanteda::docvars(corpusbic)$ave_sentiment)[3]), ])
as.character(corpusbic[ which(quanteda::docvars(corpusbic)$ave_sentiment == summary(quanteda::docvars(corpusbic)$ave_sentiment)[5]), ])
#Remarkable 2 Pen
as.character(corpusRM[ which(quanteda::docvars(corpusRM)$ave_sentiment == summary(quanteda::docvars(corpusRM)$ave_sentiment)[1]), ])
as.character(corpusRM[ which(quanteda::docvars(corpusRM)$ave_sentiment == summary(quanteda::docvars(corpusRM)$ave_sentiment)[5]), ])

```
Now, we can proceed to the feature extraction step. We will tokenize the text in each corpus, which means breaking it down into individual words or tokens. We will also remove punctuation, symbols, numbers, URLs, and stopwords from the tokens. Then, we will create a document-feature matrix (DFM) for each corpus.
```{r feature tokenization}
#BIC Cristal
tokens_bic <- quanteda::tokens(corpusbic, 
                           what = "word", # change the keyword
                           remove_punct = TRUE,
                           remove_symbols = TRUE,
                           remove_numbers = TRUE,
                           remove_url = TRUE,
                           remove_separators = TRUE,
                           split_hyphens = FALSE,
                           split_tags = FALSE,
                           include_docvars = TRUE,
                           padding = FALSE,
                           verbose = TRUE)
#Remarkable 2 Pen
tokens_RM <- quanteda::tokens(corpusRM, 
                           what = "word", # change the keyword
                           remove_punct = TRUE,
                           remove_symbols = TRUE,
                           remove_numbers = TRUE,
                           remove_url = TRUE,
                           remove_separators = TRUE,
                           split_hyphens = FALSE,
                           split_tags = FALSE,
                           include_docvars = TRUE,
                           padding = FALSE,
                           verbose = TRUE)
```

The `tokens()` function takes a corpus as its input, cleans each document and generates a vector of processed words per document. We can explore the resulting `tokens` object, for example, by sampling one of its elements:
```{r tokens sample}
tokens_bic[ sample(length(tokens_bic), 1) ]
tokens_RM[ sample(length(tokens_RM), 1) ]
```

We now have vectors per review that contain the extracted and preprocessed words per review.
Next, we can explore the tokens in each corpus. We can check the first few tokens in each corpus. We can check certain keywords in context, for example, the word "nice" in the BIC Cristal corpus. Also keywords in context for the ReMarkable 2 pen corpus, for example, the word "good". We can also convert the tokens to lowercase to ensure consistency in the analysis.
```{r keywords in context}
#BIC Cristal
data.frame(quanteda::kwic(tokens_bic, pattern = "nice"))[ c(1:5), c(4:6) ] #Change the word nice
tokens_bic <- quanteda::tokens_tolower(tokens_bic)
#Remarkable 2 Pen
data.frame(quanteda::kwic(tokens_RM, pattern = "good"))[ c(1:5), c(4:6) ] #Change the word good
tokens_RM <- quanteda::tokens_tolower(tokens_RM)
```

Now, we can remove stopwords from the tokens. Stopwords are common words that do not carry much meaning, such as "the", "and", "is", etc. We will use the English stopwords list provided by the `quanteda` package.
```{r explore stopwords}
#BIC Cristal
quanteda::stopwords("en")
tokens_bic <- quanteda::tokens_remove(tokens_bic, pattern = stopwords("en"))
#Remarkable 2 Pen
quanteda::stopwords("en")
tokens_RM <- quanteda::tokens_remove(tokens_RM, pattern = stopwords("en"))
```

We have removed the stopwords from the tokens, now, before creating dfm for each corpus, we can do lemmatization. In this way, we are able to reduce words to their basic form while considering their meaning based on their part in the speech. To do so, we first install the required packages.
```{r}
# Install additional required packages
packages <- c(packages, "textstem", "lexicon")  # Add new packages
for (i in packages) {
    if(!require(i, character.only = TRUE)) {
        install.packages(i, dependencies = TRUE)
    }
}
library(textstem)
```

Now we can actually do lemmization.
```{r}
# Function to lemmatize tokens while preserving POS context
lemmatize_tokens <- function(tokens) {
    # Convert tokens to list for processing
    token_list <- as.list(tokens)
    
    # Lemmatize each document
    lemmatized <- lapply(token_list, function(x) {
        if (length(x) == 0) return(x)  # Handle empty documents
        lemmas <- textstem::lemmatize_words(x, dictionary = lexicon::hash_lemmas)
        return(lemmas)
    })
    
    # Convert back to tokens object
    new_tokens <- quanteda::as.tokens(lemmatized)
    quanteda::docvars(new_tokens) <- quanteda::docvars(tokens)
    return(new_tokens)
}

# Create copies of tokens before lemmatization
tokens_bic_orig <- tokens_bic
tokens_RM_orig <- tokens_RM

# Apply lemmatization to both token sets
tokens_bic <- lemmatize_tokens(tokens_bic)
tokens_RM <- lemmatize_tokens(tokens_RM)
```

Now we add a verification step to check results.
```{r}
# Check lemmatization package functionality
cat("\nLEMMATIZATION PACKAGE CHECK:\n")
test_words <- c("running", "ran", "runs", "better", "best", "is", "was")
test_lemmas <- textstem::lemmatize_words(test_words, dictionary = lexicon::hash_lemmas)
data.frame(Word = test_words, Lemma = test_lemmas)
```
It works as desired.

Now we can proceed with the document-feature matrix creation. A document-feature matrix (DFM) is a matrix that represents the frequency of each word (feature) in each document. We will use the `quanteda` package to create the DFM.
```{r dfm creation}
#BIC Cristal
dfm_bic <- quanteda::dfm(tokens_bic)
dfm_bic
topfeatures(dfm_bic)
summary(dfm_bic)
#Remarkable 2 Pen
dfm_RM <- quanteda::dfm(tokens_RM)
dfm_RM
topfeatures(dfm_RM)
summary(dfm_RM)
```

Next we will perform cleaning, but firstly we will check the length of the tokens in each corpus. This will help us to identify any tokens that are too short, which might not be useful for our analysis.
```{r filter token length}
#BIC Cristal
length_features_bic <- data.frame(count = quanteda::topfeatures(dfm_bic, ncol(dfm_bic))) # create data frame 
length_features_bic$length <- nchar(rownames(length_features_bic)) # extract token length
length_features_bic[ which(length_features_bic$length %in% c(1, 2)), ] # filter on token length
#Remarkable 2 Pen
length_features_RM <- data.frame(count = quanteda::topfeatures(dfm_RM, ncol(dfm_RM))) # create data frame
length_features_RM$length <- nchar(rownames(length_features_RM)) # extract token length
length_features_RM[ which(length_features_RM$length %in% c(1, 2)), ] # filter on token length
```

Based on the result above on both BIC Cristal and Remarkable 2 Pen, we can see that there are no significant concerns, so we can proceed to the next step.

Now we create wordcloud for both brands
```{r wordcloud}

library(wordcloud)
library(RColorBrewer)
#BIC Cristal
# Create a term frequency table
freq <- colSums(dfm_bic)
freq <- sort(freq, decreasing = TRUE)

# Basic word cloud
wordcloud(words = names(freq), freq = freq, min.freq = 5,
          max.words = 100, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"))
#Remarkable 2 Pen
# Create a term frequency table
freq <- colSums(dfm_RM)
freq <- sort(freq, decreasing = TRUE)

# Basic word cloud
wordcloud(words = names(freq), freq = freq, min.freq = 5,
          max.words = 100, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"))
```


## 4. TOPIC MODELING

We can now proceed to the topic modeling step. Topic modeling is a technique used to discover the underlying topics in a collection of documents. We will use the `seededlda` package to perform topic modeling on both corpora.
```{r topic modelling}
# BIC Cristal Topic Modeling
textmodel_positive_bic <- seededlda::textmodel_lda(dfmbic[ docvars(dfmbic)$ave_sentiment >= 0, ], k = 2)
textmodel_negative_bic <- seededlda::textmodel_lda(dfmbic[ docvars(dfmbic)$ave_sentiment < 0, ], k = 2)
# ReMarkable 2 Pen Topic Modeling
textmodel_positive_rm <- seededlda::textmodel_lda(dfmRM[ docvars(dfmRM)$ave_sentiment >= 0, ], k = 2)
textmodel_negative_rm <- seededlda::textmodel_lda(dfmRM[ docvars(dfmRM)$ave_sentiment < 0, ], k = 2)
```

## 5. INTERPRETATION AND EVALUATION
We can now interpret the results of the topic modeling. We can check the top terms for each topic in both corpora.
```{r document distribution}
#BIC Cristal
head(topics(textmodel_positive_bic))
head(topics(textmodel_negative_bic))
table(topics(textmodel_positive_bic))
table(topics(textmodel_negative_bic))
#Remarkable 2 Pen
head(topics(textmodel_positive_rm))
head(topics(textmodel_negative_rm))
table(topics(textmodel_positive_rm))
table(topics(textmodel_negative_rm))
```
Based on the result above. in BIC Cristal, most documents are linked to topic 1, while in ReMarkable 2 Pen, most documents are more varied linked to topic 1 and 2. 
Let's now check the top terms for each topic in both corpora. This will help us to understand the topics better and see if they make sense.

``` {r topic model interpretation}
#BIC Cristal
cbind(seededlda::terms(textmodel_positive_bic, 10), 
      seededlda::terms(textmodel_negative_bic, 10))
#Remarkable 2 Pen
cbind(seededlda::terms(textmodel_positive_rm, 10), 
      seededlda::terms(textmodel_negative_rm, 10))
```
The first two column represent the positive sentiment, while the others are negative sentiment. As we can wee here, the topics in BIC Cristal are related to the quality of the pen, its price, and its availability. While in ReMarkable 2 Pen, the topics are related to the quality of the pen, its price, and its usability. But we notice there are words that might be not meaningful in understanding the customers review such as pen, pens, cristal, etc. We will improve the model performance

## 6. MODEL IMPROVEMENT
First, removing short review

```{r remove short reviews}
# BIC Cristal
## Remove documents that contain less than 1 words, I tried for 5, it removes everything
tokens_bic_new <- tokens_bic[ quanteda::ntoken(tokens_bic) >= 2 ]

## Prepare the new dfm (this integrates some of the previous steps)
dfm_bic <- quanteda::dfm(tokens_bic_new)
#dfm_bic <- dfm_trim(dfm, min_termfreq = 3)
#dfm_bic <- dfm[ -which((rowSums(dfm)) %in% 0), ]
summary(dfm_bic)
## Run the new textmodels
textmodel_positive_bic_new <- seededlda::textmodel_lda(dfm_bic[ docvars(dfm_bic)$ave_sentiment >= 0, ], k = 2)
textmodel_negative_bic_new <- seededlda::textmodel_lda(dfm_bic[ docvars(dfm_bic)$ave_sentiment < 0, ], k = 2)

## Explore the topics
cbind(seededlda::terms(textmodel_positive_bic_new), 
      seededlda::terms(textmodel_negative_bic_new))

# ReMarkable 2 Pen
## Remove documents that contain less than 2 words
tokens_RM_new <- tokens_RM[ quanteda::ntoken(tokens_RM) >= 2 ]
## Prepare the new dfm (this integrates some of the previous steps)
dfm_RM <- quanteda::dfm(tokens_RM_new)
#dfm_RM <- dfm_trim(dfm_RM, min_termfreq = 10)
#dfm_RM <- dfm_RM[ -which((rowSums(dfm_RM)) %in% 0), ]
## Run the new textmodels
summary(dfm_RM)
textmodel_positive_RM_new <- seededlda::textmodel_lda(dfm_RM[ docvars(dfm_RM)$ave_sentiment >= 0, ], k = 2)
textmodel_negative_RM_new <- seededlda::textmodel_lda(dfm_RM[ docvars(dfm_RM)$ave_sentiment < 0, ], k = 2)

## Explore the topics
cbind(seededlda::terms(textmodel_positive_RM_new), 
      seededlda::terms(textmodel_negative_RM_new))
```
Trying model adjustment
First, let's check the BIC Cristal model, we will try to adjust the model by changing the number of topics and the parameters of the LDA model. We will also check if the topics make sense and if they are interpretable.
```{r model adjustments}
##BIC Cristal
textmodel_positive_bic_new <- seededlda::textmodel_lda(dfm_bic[ docvars(dfm_bic)$ave_sentiment >= 0, ],
                                                   k = 2,
                                                   max_iter = 1000,
                                                   alpha = 0.5,
                                                   beta = 0.05) 
textmodel_negative_bic_new <- seededlda::textmodel_lda(dfm_bic[ docvars(dfm_bic)$ave_sentiment < 0, ],
                                                   k = 2,
                                                   max_iter = 1000,
                                                   alpha = 0.5,
                                                   beta = 0.05)
cbind(seededlda::terms(textmodel_positive_bic_new), 
      seededlda::terms(textmodel_negative_bic_new))
```

Based on model adjustment, no significant changes on the topics, so we can proceed to the next step.Notice, there are still words that are not relevant in understanding the customers review such as pen, pens, cristal, etc. We will improve the model performance by removing those words.

```{r manual topic filtering}
dfm_tmp_bic <- quanteda::dfm_remove(dfm_bic, pattern = c("pen", "also", "pens",
                                                 "just", "ballpoints","find", "Cristal","ballpoint","use","write","writing","bic","s","want","can"  ))

textmodel_positive_bic_new <- seededlda::textmodel_lda(dfm_tmp_bic[ docvars(dfm_tmp_bic)$ave_sentiment >= 0, ], k = 2)
textmodel_negative_bic_new <- seededlda::textmodel_lda(dfm_tmp_bic[ docvars(dfm_tmp_bic)$ave_sentiment < 0, ], k = 2)
cbind(terms(textmodel_positive_bic_new), terms(textmodel_negative_bic_new))
```



```{r}
library(wordcloud)
library(RColorBrewer)

# Create a term frequency table
freq <- colSums(dfm_tmp_bic)
freq <- sort(freq, decreasing = TRUE)

# Basic word cloud
wordcloud(words = names(freq), freq = freq, min.freq = 5,
          max.words = 100, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"))
```
Now, we can check the ReMarkable 2 pen model, we will try to adjust the model by changing the number of topics and the parameters of the LDA model. We will also check if the topics make sense and if they are interpretable.

```{r model adjustments}
##Remarkable 2 Pen
textmodel_positive_RM_new <- seededlda::textmodel_lda(dfm_RM[ docvars(dfm_RM)$ave_sentiment >= 0, ],
                                                   k = 2,
                                                   max_iter = 1000,
                                                   alpha = 0.5,
                                                   beta = 0.05) 
textmodel_negative_RM_new <- seededlda::textmodel_lda(dfm_RM[ docvars(dfm_RM)$ave_sentiment < 0, ],
                                                   k = 2,
                                                   max_iter = 1000,
                                                   alpha = 0.5,
                                                   beta = 0.05)
cbind(seededlda::terms(textmodel_positive_RM_new), 
      seededlda::terms(textmodel_negative_RM_new))
```

Based on model adjustment, no significant changes on the topics, so we can proceed to the next step. Notice, there are still words that are not relevant in understanding the customers review such as pen, pens, remarkable, etc. We will improve the model performance by removing those words.

```{r manual topic filtering}
dfm_tmp_RM <- quanteda::dfm_remove(dfm_RM, pattern = c("pen", "also", "pens",
                                                 "just", "much","find", "lamy","rm","rm2","see","really","writing","s","want","can","pencil","stylus","nib","nibs","back","think","rm1","amazon","kindle"  ))

textmodel_positive_RM_new <- seededlda::textmodel_lda(dfm_tmp_RM[ docvars(dfm_tmp_RM)$ave_sentiment >= 0, ], k = 2)
textmodel_negative_RM_new <- seededlda::textmodel_lda(dfm_tmp_RM[ docvars(dfm_tmp_RM)$ave_sentiment < 0, ], k = 2)
cbind(terms(textmodel_positive_RM_new), terms(textmodel_negative_RM_new))
```
```{r}
library(wordcloud)
library(RColorBrewer)

# Create a term frequency table
freq <- colSums(dfm_tmp_RM)
freq <- sort(freq, decreasing = TRUE)

# Basic word cloud
wordcloud(words = names(freq), freq = freq, min.freq = 5,
          max.words = 100, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"))
```

